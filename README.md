쪼우
jjowoo
오프라인 표시



조재현 — 2025-05-27 오후 5:58
사용자 설정 가서
음성 및 비디어
오
출력 장치
조재현 — 2025-05-27 오후 6:22
✔
조재현 — 2025-07-11 오후 10:41
채팅보임?
조재현 — 2025-07-14 오후 3:06
https://docs.google.com/spreadsheets/d/14pDOB_BpLAVN_DzaZum5-PIlJzWOcJE7IoHwB0149Ic/edit?gid=1340085368#gid=1340085368
Google Docs
데이터셋 현황
https://www.notion.so/25-07-13-22f5bb5dbbec80e9b90cf9112fb8a95e?source=copy_link
전민 — 2025-07-18 오후 2:29
내가 프론트 실행해볼테니까 형이 보고 형이 생각하는대로 동작하는지 확인해줄래?
조재현 — 2025-08-13 오후 3:28
https://www.figma.com/design/AVvdgO1QmPC1Wqvy2yZ60d/Untitled?node-id=1060-2&t=PT3mQ1Jk4G9fXaBu-1
조재현 — 2025-08-13 오후 3:49
이미지
전민 — 2025-08-19 오전 2:13
Probmap을 도입하여 기존 heatmap 방식 대비 이미지 밖에 있는 신체 부위 예측 능력을 향상.
전민 — 2025-08-19 오전 2:24
https://arxiv.org/pdf/2309.07910
조재현 — 2025-08-19 오전 2:33
https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_RTMO_Towards_High-Performance_One-Stage_Real-Time_Multi-Person_Pose_Estimation_CVPR_2024_paper.pdf?utm_source=chatgpt.com
전민 — 2025-08-19 오전 2:45
x축과 y축을 독립적인 1차원 벡터로 분리한 뒤 각 차원별로 좌표의 확률을 탐색하여 연산 효율과 정확도를 동시에 높임
조재현 — 2025-08-19 오전 2:46
Vision Transformer를 기반으로 포즈 추정 모델. 대규모 데이터와 사전 학습된 비전 트랜스포머를 활용하여 범용성과 확장성을 보유.
YOLO 객체 탐지기에 keypoint regression을 결합하여 탐지와 포즈 추정을 동시에 수행하는 one-stage 방식의 모델. 단일 네트워크로 포즈 추정을 처리할 수 있어, 모바일 환경 적용이 가능.
전민 — 2025-08-19 오전 2:50
YOLO 객체 탐지기에 x, y 좌표를 동시에 탐색하는 것이 아닌 각 차원별 좌표 확률을 독립적으로 탐색하는 연산 효율과 정확도를 향상시킨 one-stage 방식의 모델.
YOLO 객체 탐지기에 x, y 좌표를 동시에 탐색하는 것이 아닌 각 차원별 좌표 확률을 독립적으로 탐색하는 연산 효율과 정확도를 향상시킨 one-stage 방식의 모델.
전민 — 2025-08-19 오전 3:10
MobileNet 아키텍처를 기반으로 deconvolution, large kernel convolution를 통해 모바일 환경에서의 포즈 추정이 가능한 경량 CNN 모델
MovePose: A High-performance Human Pose
Estimation Algorithm on Mobile and Edge
Devices
조재현 — 2025-08-19 오전 3:12
첨부 파일 형식: document
추가 작성v2_ANAPA_향후연구_0818_v1.pptx
5.91 MB
전민 — 2025-08-25 오후 2:37
이미지
전민 — 2025-08-29 오전 12:16
Each step is visually organized, allowing users to easily train and analyze performance within a quantum split learning environment.
전민 — 어제 오후 11:32
riceCakeSsamanKo
조재현 — 어제 오후 11:34
https://github.com/HG-AISLAB/VELCRO
GitHub
GitHub - HG-AISLAB/VELCRO
Contribute to HG-AISLAB/VELCRO development by creating an account on GitHub.
GitHub - HG-AISLAB/VELCRO
전민 — 어제 오후 11:49
git add .
git commit -m "Init Backend"
git push -f origin main
전민 — 오후 11:44
# **QSPLIT: Quantum Split Learning Testing Toolkit**



## **Tool Components**
![](https://cdn.discordapp.com/attachments/1149758290566324276/1413580984791728210/Fig3_5.png?ex=68bc7361&is=68bb21e1&hm=c14ab1c6fe1d8323752827ade7ba7a90948a7148456835e33bb7e9d758c05bc2&)
확장
README.md
5KB
﻿
# **QSPLIT: Quantum Split Learning Testing Toolkit**



## **Tool Components**
![](https://cdn.discordapp.com/attachments/1149758290566324276/1413580984791728210/Fig3_5.png?ex=68bc7361&is=68bb21e1&hm=c14ab1c6fe1d8323752827ade7ba7a90948a7148456835e33bb7e9d758c05bc2&)

### **A. Part Selection**
The *Part Selection* interface allows users to choose which components of a QNN architecture (**SE, PQC, MEA**) will be provided as **Target Code** and which will be automatically generated as **Dummy Code**.  
- **Target Code**: User-provided code uploaded through the interface  
- **Dummy Code**: Remaining components are automatically generated by QSPLIT  



### **B. Target Code Hyperparameter**
The *Target Code Hyperparameter* interface allows users to configure hyperparameters required for quantum split learning. It consists of two sections:  
- **Quantum Device**: Parameters for dummy code generation, such as number of qubits, batch size, and execution device  
- **Training**: Parameters for the training process, such as number of epochs, optimizer, and learning rate  



### **C. Dummy Code Generation**
The *Dummy Code Generation* interface generates dummy codes based on the uploaded target code and configured parameters.  
- The number of generated dummy codes is determined by the `Number of Dummy Codes` hyperparameter  
- **Dummy List**: Displays the generated dummy codes in the GUI  
- Users can select each dummy code to inspect its internal structure (e.g., PQC layers and MEA measurements)  



### **D. Split Learning Execution**
By clicking the **Run** button in the *Dummy Code Generation* interface, users can initiate split learning across multiple target-dummy combinations.  
- The training process is executed using the **TorchQuantum framework**  
- **Real-time training logs** (e.g., loss, accuracy) are displayed in the GUI, allowing users to monitor progress  



### **E. Result Visualization & Export**
The *Results* interface displays the classification accuracy for each combination after split learning is completed.  
- Users can select a desired dummy code and click the **Export** button to save it as an executable `.py` file  
- Exported code is compatible with the **TorchQuantum** framework  
- This allows validated QNN components to be reused for **subsequent development**  

---

## **Key Features**
- **Automated Dummy Code Generation**  
- **Split Learning Execution**  
- **Real-time Log & Result Presentation**  
- **Code Export for Reusability**  

---

# HOW TO USE

## 1. How to Start

### 1) System Prerequisites
- **Python** ≥ 3.12  
- **Uvicorn**  
- **PyTorch** (tested with 2.8.0)  
- **TorchQuantum** (tested with 0.1.8)  
- **Flutter** 3.32.8 (for frontend)  
- A modern web browser (Chrome/Edge/Firefox)  

### 2) Setup
```bash
# create & activate venv (example)
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```
```bash
# run backend
cd Backend
uvicorn app.main:app --host 0.0.0.0 --port 8000
```
```bash
# run frontend
cd Frontend
flutter run -d chrome
```

---

## 2. Configure Components

- **Upload Target Code**: In *Part Selection*, click **Upload** to provide any subset of SE / PQC / MEA as target code.  
- **Assign Roles**: Select each component as **Target Code** or **Dummy Code** (radio buttons; mutually exclusive).  
- **Parameter Input**: In *Target Code Hyperparameter*  
  - *Quantum Device*: number of qubits, batch size, execution device  
  - *Training*: epochs, optimizer, learning rate  
  - *Dataset*: choose from datasets uploaded to QSPLIT (e.g., MedNIST)  

---

## 3. Generate Dummy Code

1. Configure parameters in *Target Code Hyperparameter*  
2. Click **Generate** to create dummy code  
3. Inspect in *Dummy Code Generation → Dummy List*  
   - Example: PQC = RY/RZ/CNOT gate stack, MEA = Z observable  

---

## 4. Execute Split Learning

- In *Dummy Code Generation*, click **Run** to start split learning across all combinations  
- Training uses *TorchQuantum*; per-epoch loss/accuracy are streamed in real time to the GUI  

---

## 5. Compare Results

- *Results* interface lists accuracy and metrics per combination  
- Easily identify stable or high-performing combinations  

---

## 6. Export Code

- In *Results*, select a dummy code and click **Export**  
- Outputs an executable `.py` file compatible with *TorchQuantum*  
- Enables reuse, extension, and integration into follow-up experiments or deployment  

---

## Software Version
### v1.0.0  
README.md
5KB
